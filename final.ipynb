{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import community\n",
    "import networkx as nx\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Q3.1\n",
    "\n",
    "# parsing data from url\n",
    "small_data_url = urllib.request.urlopen(\"https://liacs.leidenuniv.nl/~takesfw/SNACS/twitter-small.tsv\")\n",
    "large_data_url = urllib.request.urlopen(\"https://liacs.leidenuniv.nl/~takesfw/SNACS/twitter-larger.tsv\")\n",
    "\n",
    "small_data = small_data_url.read().decode('utf-8')  # str\n",
    "large_data = large_data_url.read().decode('utf-8')\n",
    "\n",
    "lines = small_data.split('\\n')\n",
    "mentioned_users_list = {}\n",
    "\n",
    "for index, line in enumerate(lines):  \n",
    "\n",
    "    parts = line.strip().split('\\t')\n",
    "    if len(parts) >= 3:\n",
    "\n",
    "        date, username, tweet = parts[0], parts[1], parts[2]\n",
    "\n",
    "        print(\"Line no: \", index + 1)\n",
    "        print(\"Date: \", date)\n",
    "        print(\"Username: \", username)\n",
    "        print(\"Tweet: \", tweet)\n",
    "\n",
    "        words = tweet.split()\n",
    "        mentioned_users = []\n",
    "\n",
    "        for word in words:\n",
    "            if word.startswith(\"@\"):\n",
    "                mentioned_user = word[1:]  # It did not work really, some of the nodes still has @ at beginning\n",
    "                mentioned_users.append(mentioned_user)\n",
    "\n",
    "        mentioned_users_list[username] = mentioned_users\n",
    "\n",
    "        # print mentioned users for the current user\n",
    "        if mentioned_users:\n",
    "            print(username, \" mentioned: \", \", \".join(mentioned_users))\n",
    "            print(len(mentioned_users))\n",
    "\n",
    "        print(\"-----------------------------------------------------\")\n",
    "\n",
    "    else:\n",
    "        print(\"Line no: \", index + 1, \"Tweet does not have tab spaces that I can split!!!\")\n",
    "        # last line has a new line after it that's why i added this\n",
    "directedMentionGraph = nx.DiGraph()\n",
    "\n",
    "for user, mentions in mentioned_users_list.items():\n",
    "    for mentioned_user in mentions:\n",
    "        if not directedMentionGraph.has_edge(user, mentioned_user):\n",
    "            directedMentionGraph.add_edge(user, mentioned_user, weight=1)\n",
    "        else:\n",
    "            directedMentionGraph[user][mentioned_user][\"weight\"] += 1\n",
    "\n",
    "# csv file for the edges\n",
    "with open(\"weighted_edge_list.csv\", mode='w', newline='', encoding=\"utf-8\") as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow([\"Source\", \"Target\", \"Weight\"])\n",
    "    for edge in directedMentionGraph.edges(data=True):\n",
    "        source, target, data = edge\n",
    "        weight = data['weight']\n",
    "        writer.writerow([source, target, weight])\n",
    "\n",
    "print(\"\\n************ E N D     O F    Q U E S T I O N 1 ************\")\n",
    "\n",
    "# Q3.2\n",
    "# nodes\n",
    "# print(\"Nodes: \", directedMentionGraph.nodes())\n",
    "print(\"No of nodes: \", len(directedMentionGraph.nodes()))\n",
    "# edges\n",
    "# print(\"Edges: \", directedMentionGraph.edges())\n",
    "print(\"No of edges: \", len(directedMentionGraph.edges()))\n",
    "\n",
    "# strongly connected comp\n",
    "print(\"Strongly connected components: \", nx.strongly_connected_components(directedMentionGraph))\n",
    "strongly_connected_components = list(nx.strongly_connected_components(directedMentionGraph))\n",
    "print(\"Number of strongy connected components: \", len(strongly_connected_components))\n",
    "\n",
    "for i, component in enumerate(strongly_connected_components):\n",
    "    print(\"Size of \", i + 1, \". strongly connected component : \", len(component))\n",
    "\n",
    "# weakly connected comp\n",
    "print(\"Weakly connected components: \", nx.weakly_connected_components(directedMentionGraph))\n",
    "weakly_connected_components = list(nx.weakly_connected_components(directedMentionGraph))\n",
    "print(\"Number of weakly connected components: \", len(weakly_connected_components))\n",
    "\n",
    "for i, component in enumerate(strongly_connected_components):\n",
    "    print(\"Size of \", i + 1, \". weakly connected component : \", len(component))\n",
    "\n",
    "# density = 2m/n.(n-1)\n",
    "density = (2 * len(directedMentionGraph.edges())) / (\n",
    "        len(directedMentionGraph.nodes()) * (len(directedMentionGraph.nodes()) - 1))\n",
    "print(\"Density of the network: \", density)\n",
    "\n",
    "# indegree and outdegree distributions\n",
    "indegree = dict(directedMentionGraph.in_degree())\n",
    "outdegree = dict(directedMentionGraph.out_degree())\n",
    "\n",
    "plt.hist(list(indegree.values()), bins=20, color='r')\n",
    "plt.title('Indegree Distribution')\n",
    "plt.xlabel('Indegree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(list(outdegree.values()), bins=20, color='r')\n",
    "plt.title('Outdegree Distribution')\n",
    "plt.xlabel('Outdegree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# average cc directed & undirected\n",
    "directed_average_clustering_coefficient = nx.average_clustering(directedMentionGraph)\n",
    "print(\"Directed average clustering coefficint: \", directed_average_clustering_coefficient)\n",
    "\n",
    "undirectedMentionGraph = directedMentionGraph.to_undirected()\n",
    "undirected_average_clustering_coefficient = nx.average_clustering(undirectedMentionGraph)\n",
    "print(\"Undirected average clustering coefficient: \", undirected_average_clustering_coefficient)\n",
    "\n",
    "subset_size = 1000\n",
    "all_nodes = list(undirectedMentionGraph.nodes())\n",
    "\n",
    "subsets = []\n",
    "for i in range(0, len(all_nodes), subset_size):\n",
    "    subset_nodes = all_nodes[i:i + subset_size]\n",
    "    subset_graph = undirectedMentionGraph.subgraph(subset_nodes)\n",
    "    subsets.append(subset_graph)\n",
    "\n",
    "giant_components_sub = []\n",
    "avg_distance_list = []\n",
    "avg_value = 0\n",
    "for subset in subsets:\n",
    "    connected_components = list(nx.connected_components(subset))\n",
    "    if connected_components:\n",
    "        giant_component = max(connected_components, key=len)\n",
    "        giant_components_sub.append(subset.subgraph(giant_component))\n",
    "\n",
    "for i, giant_component in enumerate(giant_components_sub):\n",
    "    average_distance = nx.average_shortest_path_length(giant_component)\n",
    "    avg_value += average_distance\n",
    "    avg_distance_list.append(average_distance)\n",
    "\n",
    "print(\"Average Distance in giant component: \", avg_value / len(avg_distance_list))\n",
    "\n",
    "giant_component = max(nx.connected_components(undirectedMentionGraph), key=len)\n",
    "giant_subgraph = undirectedMentionGraph.subgraph(giant_component)\n",
    "\n",
    "sample_size = 50  # Number of nodes to sample\n",
    "distance_distribution = {}  # Dictionary to store distance distribution\n",
    "\n",
    "# Convert NodeView to a list for sampling\n",
    "sampled_nodes = random.sample(list(giant_subgraph.nodes()), sample_size)\n",
    "\n",
    "# Calculate distances for each sampled node\n",
    "for source_node in sampled_nodes:\n",
    "    distances = nx.single_source_shortest_path_length(giant_subgraph, source_node)\n",
    "\n",
    "    # Update the distance_distribution dictionary\n",
    "    for distance in distances.values():\n",
    "        if distance in distance_distribution:\n",
    "            distance_distribution[distance] += 1\n",
    "        else:\n",
    "            distance_distribution[distance] = 1\n",
    "\n",
    "# Convert the counts to frequencies\n",
    "total_counts = sum(distance_distribution.values())\n",
    "distance_distribution = {distance: count / total_counts for distance, count in distance_distribution.items()}\n",
    "\n",
    "plt.hist(list(distance_distribution.values()), bins=20, color='r')\n",
    "plt.title('Distance Distribution of the Giant component')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n************ E N D     O F    Q U E S T I O N 2 ************\")\n",
    "\n",
    "# Q3.3\n",
    "\n",
    "top_mentions = []\n",
    "\n",
    "# eigen vector centrality\n",
    "\n",
    "ei_centrality = nx.eigenvector_centrality(undirectedMentionGraph)\n",
    "direct_ei_centrality = nx.eigenvector_centrality(directedMentionGraph)\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Eigen Vector Centrality (Undirected Graph) : \\n\")\n",
    "\n",
    "i = 0\n",
    "while i < 20:\n",
    "    top_ei_node = max(ei_centrality, key=ei_centrality.get)\n",
    "    top_ei_value = ei_centrality.pop(top_ei_node)\n",
    "    print((i + 1), \". user's name: @\", top_ei_node, \" - value: \", top_ei_value)\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Eigen Vector Centrality (Directed Graph) : \\n\")\n",
    "\n",
    "i = 0\n",
    "while i < 20:\n",
    "    top_dei_node = max(direct_ei_centrality, key=direct_ei_centrality.get)\n",
    "    top_dei_value = direct_ei_centrality.pop(top_dei_node)\n",
    "    print((i + 1), \". user's name: @\", top_dei_node, \" - value: \", top_dei_value)\n",
    "    i += 1\n",
    "\n",
    "# closeness centrality\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Closeness Centrality (Undirected graph): \\n\")\n",
    "\n",
    "ucloseness_centrality = nx.closeness_centrality(undirectedMentionGraph)\n",
    "# that was taking ages so i found parallel centrality to fix it instead of sampling because\n",
    "# i needed to find the exact top 20\n",
    "\n",
    "i = 0\n",
    "while i < 20:\n",
    "    top_ucloseness_node = max(ei_centrality, key=ucloseness_centrality.get)\n",
    "    top_ucloseness_value = ei_centrality.pop(top_ucloseness_node)\n",
    "    print((i + 1), \". user's name: @\", top_ucloseness_node, \" - value: \", top_ucloseness_value)\n",
    "    i += 1\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Closeness Centrality (In degree): \\n\")\n",
    "\n",
    "dcloseness_centrality = nx.closeness_centrality(directedMentionGraph, distance='in')\n",
    "\n",
    "for i in range(20):\n",
    "    top_closeness_node = max(dcloseness_centrality, key=dcloseness_centrality.get)\n",
    "    top_closeness_value = dcloseness_centrality[top_closeness_node]\n",
    "    if top_closeness_node not in top_mentions:\n",
    "        top_mentions.append(top_closeness_node)\n",
    "        print((i + 1), \". user's name: @\", top_closeness_node, \" - value: \", top_closeness_value)\n",
    "        del dcloseness_centrality[top_closeness_node]\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Closeness Centrality (Out degree): \\n\")\n",
    "\n",
    "doutcloseness_centrality = nx.closeness_centrality(directedMentionGraph, distance='out')\n",
    "\n",
    "for i in range(20):\n",
    "    top_outcloseness_node = max(doutcloseness_centrality, key=doutcloseness_centrality.get)\n",
    "    top_outcloseness_value = doutcloseness_centrality[top_outcloseness_node]\n",
    "    if top_outcloseness_node not in top_mentions:\n",
    "        top_mentions.append(top_outcloseness_node)\n",
    "        print((i + 1), \". user's name: @\", top_outcloseness_node, \" - value: \", top_outcloseness_value)\n",
    "        del doutcloseness_centrality[top_outcloseness_node]\n",
    "\n",
    "# degree centarlity\n",
    "\n",
    "degree_centrality = nx.degree_centrality(undirectedMentionGraph)  # bu yonden bagimsizmis\n",
    "in_degree_centrality = nx.in_degree_centrality(directedMentionGraph)\n",
    "out_degree_centrality = nx.out_degree_centrality(directedMentionGraph)\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Degree Centrality (Undirected Graph): \\n\")\n",
    "\n",
    "for i in range(20):\n",
    "    top_degree_node = max(degree_centrality, key=degree_centrality.get)\n",
    "    top_degree_value = degree_centrality[top_degree_node]\n",
    "    top_mentions.append(top_degree_node)\n",
    "    print((i + 1), \". user's name: @\", top_degree_node, \" - value: \", top_degree_value)\n",
    "    del degree_centrality[top_degree_node]\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Degree Centrality (In Degree): \\n\")\n",
    "\n",
    "for i in range(20):\n",
    "    top_indegree_node = max(in_degree_centrality, key=in_degree_centrality.get)\n",
    "    top_indegree_value = in_degree_centrality[top_indegree_node]\n",
    "    top_mentions.append(top_indegree_node)\n",
    "    print((i + 1), \". user's name: @\", top_indegree_node, \" - value: \", top_indegree_value)\n",
    "    del in_degree_centrality[top_indegree_node]\n",
    "\n",
    "print(\"\\n Top 20 mentions - with Degree Centrality (Out Degree): \\n\")\n",
    "\n",
    "for i in range(20):\n",
    "    top_outdegree_node = max(out_degree_centrality, key=out_degree_centrality.get)\n",
    "    top_outdegree_value = out_degree_centrality[top_outdegree_node]\n",
    "    top_mentions.append(top_outdegree_node)\n",
    "    print((i + 1), \". user's name: @\", top_outdegree_node, \" - value: \", top_outdegree_value)\n",
    "    del out_degree_centrality[top_outdegree_node]\n",
    "\n",
    "print(\"\\n************ E N D     O F    Q U E S T I O N 3 ************\")\n",
    "\n",
    "# Q3.4\n",
    "\n",
    "# Louvain Algorithm\n",
    "\n",
    "giant_undirected = undirectedMentionGraph.subgraph(giant_component).to_undirected()\n",
    "\n",
    "partition = community.best_partition(giant_undirected)\n",
    "\n",
    "print(\"Result of Louvain:\", partition)\n",
    "\n",
    "print(\"\\n************ E N D     O F    Q U E S T I O N 4 ************\")\n",
    "\n",
    "# Q3.5\n",
    "\n",
    "data = []\n",
    "for node, community_id in partition.items():\n",
    "    data.append([node, community_id])\n",
    "\n",
    "with open(\"community_information.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow([\"Node\", \"Community\"])\n",
    "    csv_writer.writerows(data)\n",
    "\n",
    "# ************ E N D     O F    Q U E S T I O N 5 ************\n",
    "\n",
    "# Q3.6\n",
    "link_weights = [np.log(giant_undirected[u][v]['weight']) for u, v in giant_undirected.edges()]\n",
    "\n",
    "plt.hist(link_weights, bins=20, alpha=0.8)\n",
    "plt.xlabel('Link Weight')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Weight Distribution of Links')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n************ E N D     O F    Q U E S T I O N 6 ************\")\n",
    "\n",
    "# Q3.7\n",
    "\n",
    "large_lines = large_data.split('\\n')\n",
    "\n",
    "for i, large_line in enumerate(large_lines):\n",
    "\n",
    "    parts = large_line.strip().split('\\t')\n",
    "    if len(parts) >= 3:\n",
    "\n",
    "        date, username, tweet = parts[0], parts[1], parts[2]\n",
    "\n",
    "        print(\"Line no: \", i + 1)\n",
    "        print(\"Date: \", date)\n",
    "        print(\"Username: \", username)\n",
    "        print(\"Tweet: \", tweet)\n",
    "\n",
    "        words = tweet.split()\n",
    "        mentioned_users = []\n",
    "\n",
    "        for word in words:\n",
    "            if word.startswith(\"@\"):\n",
    "                mentioned_user = word[1:]  # It did not work really, some of the nodes still has @ at beginning\n",
    "                mentioned_users.append(mentioned_user)\n",
    "\n",
    "        mentioned_users_list[username] = mentioned_users\n",
    "\n",
    "        # print mentioned users for the current user\n",
    "        if mentioned_users:\n",
    "            print(username, \" mentioned: \", \", \".join(mentioned_users))\n",
    "            print(len(mentioned_users))\n",
    "\n",
    "        print(\"-----------------------------------------------------\")\n",
    "\n",
    "    else:\n",
    "        print(\"Line no: \", index + 1, \"Tweet does not have tab spaces that I can split!!!\")\n",
    "        # last line has a new line after it that's why i added this\n",
    "directedLargeMentionGraph = nx.DiGraph()\n",
    "\n",
    "for user, mentions in mentioned_users_list.items():\n",
    "    for mentioned_user in mentions:\n",
    "        if not directedLargeMentionGraph.has_edge(user, mentioned_user):\n",
    "            directedLargeMentionGraph.add_edge(user, mentioned_user, weight=1)\n",
    "        else:\n",
    "            directedLargeMentionGraph[user][mentioned_user][\"weight\"] += 1\n",
    "\n",
    "# nodes\n",
    "# print(\"Nodes: \", directedMentionGraph.nodes())\n",
    "print(\"No of nodes - Large dataset: \", len(directedLargeMentionGraph.nodes()))\n",
    "# edges\n",
    "# print(\"Edges: \", directedLargeMentionGraph.edges())\n",
    "print(\"No of edges - Large dataset: \", len(directedLargeMentionGraph.edges()))\n",
    "\n",
    "# strongly connected comp\n",
    "print(\"Strongly connected components - Large dataset: \", nx.strongly_connected_components(directedLargeMentionGraph))\n",
    "strongly_connected_components = list(nx.strongly_connected_components(directedLargeMentionGraph))\n",
    "print(\"Number of strongy connected components - Large dataset: \", len(strongly_connected_components))\n",
    "\n",
    "for i, component in enumerate(strongly_connected_components):\n",
    "    print(\"Size of \", i + 1, \". strongly connected component - Large dataset: \", len(component))\n",
    "\n",
    "# weakly connected comp\n",
    "print(\"Weakly connected components: \", nx.weakly_connected_components(directedLargeMentionGraph))\n",
    "weakly_connected_components = list(nx.weakly_connected_components(directedLargeMentionGraph))\n",
    "print(\"Number of weakly connected components - Large dataset: \", len(weakly_connected_components))\n",
    "\n",
    "for i, component in enumerate(strongly_connected_components):\n",
    "    print(\"Size of \", i + 1, \". weakly connected component - Large dataset: \", len(component))\n",
    "\n",
    "# density = 2m/n.(n-1)\n",
    "density = (2 * len(directedLargeMentionGraph.edges())) / (\n",
    "        len(directedLargeMentionGraph.nodes()) * (len(directedLargeMentionGraph.nodes()) - 1))\n",
    "print(\"Density of the network - Large dataset: \", density)\n",
    "\n",
    "# indegree and outdegree distributions\n",
    "indegree = dict(directedLargeMentionGraph.in_degree())\n",
    "outdegree = dict(directedLargeMentionGraph.out_degree())\n",
    "\n",
    "plt.hist(list(indegree.values()), bins=20, color='r')\n",
    "plt.title('Indegree Distribution - Large dataset')\n",
    "plt.xlabel('Indegree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(list(outdegree.values()), bins=20, color='r')\n",
    "plt.title('Outdegree Distribution - Large dataset')\n",
    "plt.xlabel('Outdegree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# average cc directed & undirected\n",
    "directed_large_average_clustering_coefficient = nx.average_clustering(directedLargeMentionGraph)\n",
    "print(\"Directed average clustering coefficint - Large dataset: \", directed_large_average_clustering_coefficient)\n",
    "\n",
    "undirectedLargeMentionGraph = directedLargeMentionGraph.to_undirected()\n",
    "undirected_large_average_clustering_coefficient = nx.average_clustering(undirectedLargeMentionGraph)\n",
    "print(\"Undirected average clustering coefficient - Large dataset: \", undirected_large_average_clustering_coefficient)\n",
    "\n",
    "subset_size = 1000\n",
    "all_nodes = list(undirectedLargeMentionGraph.nodes())\n",
    "\n",
    "subsets = []\n",
    "for i in range(0, len(all_nodes), subset_size):\n",
    "    subset_nodes = all_nodes[i:i + subset_size]\n",
    "    subset_graph = undirectedLargeMentionGraph.subgraph(subset_nodes)\n",
    "    subsets.append(subset_graph)\n",
    "\n",
    "giant_components_sub = []\n",
    "avg_distance_list = []\n",
    "avg_value = 0\n",
    "for subset in subsets:\n",
    "    connected_components = list(nx.connected_components(subset))\n",
    "    if connected_components:\n",
    "        giant_component = max(connected_components, key=len)\n",
    "        giant_components_sub.append(subset.subgraph(giant_component))\n",
    "\n",
    "for i, giant_component in enumerate(giant_components_sub):\n",
    "    average_distance = nx.average_shortest_path_length(giant_component)\n",
    "    avg_value += average_distance\n",
    "    avg_distance_list.append(average_distance)\n",
    "\n",
    "print(\"Average Distance in giant component - Large dataset: \", avg_value / len(avg_distance_list))\n",
    "\n",
    "giant_component = max(nx.connected_components(undirectedLargeMentionGraph), key=len)\n",
    "giant_subgraph = undirectedLargeMentionGraph.subgraph(giant_component)\n",
    "\n",
    "sample_size = 50  # Number of nodes to sample\n",
    "distance_distribution = {}  # Dictionary to store distance distribution\n",
    "\n",
    "# Convert NodeView to a list for sampling\n",
    "sampled_nodes = random.sample(list(giant_subgraph.nodes()), sample_size)\n",
    "\n",
    "# Calculate distances for each sampled node\n",
    "for source_node in sampled_nodes:\n",
    "    distances = nx.single_source_shortest_path_length(giant_subgraph, source_node)\n",
    "\n",
    "    # Update the distance_distribution dictionary\n",
    "    for distance in distances.values():\n",
    "        if distance in distance_distribution:\n",
    "            distance_distribution[distance] += 1\n",
    "        else:\n",
    "            distance_distribution[distance] = 1\n",
    "\n",
    "# Convert the counts to frequencies\n",
    "total_counts = sum(distance_distribution.values())\n",
    "distance_distribution = {distance: count / total_counts for distance, count in distance_distribution.items()}\n",
    "\n",
    "plt.hist(list(distance_distribution.values()), bins=20, color='r')\n",
    "plt.title('Distance Distribution of the Giant component - Large dataset')\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5bf58229dc515b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
